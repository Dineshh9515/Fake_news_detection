{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c373cb-5868-4973-b635-24e1445e288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dkmr0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dkmr0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dkmr0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Dataset shape: (1284, 16)\n",
      "Label distribution:\n",
      "label\n",
      "false          263\n",
      "mostly-true    251\n",
      "half-true      248\n",
      "barely-true    237\n",
      "true           169\n",
      "pants-fire     116\n",
      "Name: count, dtype: int64\n",
      "Binary label distribution:\n",
      "binary_label\n",
      "0    668\n",
      "1    616\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbc23d149fa4c4ba1ddb6dd13b2ea94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dkmr0\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dkmr0\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1503253c64884baaa6b05bdcf8cdf862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf212726dd34f458611d6b1b04b62c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1be329e117c4a1bb8ab47cb4ecc58fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1511a94a035449cae36d87fc133a2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\dkmr0\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training BERT model...\n",
      "Epoch 1/3\n",
      "Training loss: 0.6971156963935266\n",
      "Accuracy: 0.5719844357976653\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.66      0.61       130\n",
      "           1       0.58      0.48      0.53       127\n",
      "\n",
      "    accuracy                           0.57       257\n",
      "   macro avg       0.57      0.57      0.57       257\n",
      "weighted avg       0.57      0.57      0.57       257\n",
      "\n",
      "Epoch 2/3\n",
      "Training loss: 0.5373813069783724\n",
      "Accuracy: 0.5914396887159533\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65       130\n",
      "           1       0.62      0.44      0.52       127\n",
      "\n",
      "    accuracy                           0.59       257\n",
      "   macro avg       0.60      0.59      0.58       257\n",
      "weighted avg       0.60      0.59      0.58       257\n",
      "\n",
      "Epoch 3/3\n",
      "Training loss: 0.2819602919312624\n",
      "Accuracy: 0.5875486381322957\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.66      0.62       130\n",
      "           1       0.60      0.51      0.55       127\n",
      "\n",
      "    accuracy                           0.59       257\n",
      "   macro avg       0.59      0.59      0.58       257\n",
      "weighted avg       0.59      0.59      0.59       257\n",
      "\n",
      "\n",
      "Statement: The United States has the highest tax rate in the world.\n",
      "Prediction: Not Fake (Confidence: 0.58)\n",
      "\n",
      "Enter statements to check (type 'exit' to quit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  This new miracle supplement cured my diabetes in just three days! I lost 20 pounds without changing my diet or exercising. Every doctor should be prescribing this!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fake (Confidence: 0.77)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  I started taking this supplement as recommended by my doctor. Over the past few weeks, I’ve noticed a slight improvement in my energy levels. It’s not a miracle, but it seems to be helping when combined with proper diet and exercise.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not Fake (Confidence: 0.82)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  This phone lasts for 10 days on a single 5-minute charge and has better features than any iPhone or Samsung ever made! It's basically free because it pays for itself in rewards!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not Fake (Confidence: 0.90)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">   Over-the-top claims, unrealistic battery life, and vague buzzwords are typical red flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not Fake (Confidence: 0.90)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  The battery life on this phone is decent—I can usually get through a full day with moderate use. The camera performs well in good lighting, and the interface is user-friendly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not Fake (Confidence: 0.92)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  This product is absolutely amazing! It saved my life! I can't believe how much I love it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fake (Confidence: 0.59)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    df.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', \n",
    "                 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', \n",
    "                 'pants_fire_counts', 'context']\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def binarize_labels(label):\n",
    "    if label in ['pants-fire', 'false', 'barely-true']:\n",
    "        return 1  # Fake\n",
    "    else:\n",
    "        return 0  # Not fake\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    df['processed_statement'] = df['statement'].apply(preprocess_text)\n",
    "    # Convert labels to binary\n",
    "    df['binary_label'] = df['label'].apply(binarize_labels)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Custom dataset class for BERT\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n",
    "\n",
    "def predict_fake_news(statement, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    processed_statement = preprocess_text(statement)\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        processed_statement,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    prediction = preds.item()\n",
    "    confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    result = \"Fake\" if prediction == 1 else \"Not Fake\"\n",
    "    \n",
    "    return result, confidence\n",
    "\n",
    "def main(file_path, test_statement=None):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(\"Loading and preparing data...\")\n",
    "    df = load_data(file_path)\n",
    "    df = prepare_dataset(df)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "    print(f\"Binary label distribution:\\n{df['binary_label'].value_counts()}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        df['processed_statement'], \n",
    "        df['binary_label'], \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Load the BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = NewsDataset(train_texts.values, train_labels.values, tokenizer)\n",
    "    test_dataset = NewsDataset(test_texts.values, test_labels.values, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "    total_steps = len(train_loader) * 3 \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nTraining BERT model...\")\n",
    "    for epoch in range(3):\n",
    "        print(f\"Epoch {epoch + 1}/3\")\n",
    "        train_loss = train_model(model, train_loader, optimizer, scheduler, device)\n",
    "        print(f\"Training loss: {train_loss}\")\n",
    "        \n",
    "        accuracy, report = evaluate_model(model, test_loader, device)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Classification Report:\\n{report}\")\n",
    "    \n",
    "    if test_statement:\n",
    "        result, confidence = predict_fake_news(test_statement, model, tokenizer, device)\n",
    "        print(f\"\\nStatement: {test_statement}\")\n",
    "        print(f\"Prediction: {result} (Confidence: {confidence:.2f})\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"valid.csv\"  \n",
    "    \n",
    "    \n",
    "    model, tokenizer = main(file_path)\n",
    "   \n",
    "    test_statement = \"The United States has the highest tax rate in the world.\"\n",
    "    result, confidence = predict_fake_news(test_statement, model, tokenizer, torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    print(f\"\\nStatement: {test_statement}\")\n",
    "    print(f\"Prediction: {result} (Confidence: {confidence:.2f})\")\n",
    "    \n",
    "    print(\"\\nEnter statements to check (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        user_input = input(\"> \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        result, confidence = predict_fake_news(user_input, model, tokenizer, torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        print(f\"Prediction: {result} (Confidence: {confidence:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cc0d61-bf70-4a75-8cf4-147f44947964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.adamw import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08afea9-a03f-4f80-ab4d-747bf2dc12f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
